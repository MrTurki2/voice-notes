import { NextRequest, NextResponse } from 'next/server';
import { GoogleGenerativeAI } from '@google/generative-ai';

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '');

export async function POST(request: NextRequest) {
  const startTime = Date.now();

  try {
    const formData = await request.formData();
    const audioFile = formData.get('audio') as File;
    const imageFile = formData.get('image') as File | null;
    const contextText = formData.get('context') as string | null;
    const mode = formData.get('mode') as string || 'accurate'; // accurate, summary, keywords

    if (!audioFile) {
      return NextResponse.json({ success: false, error: 'No audio file provided' }, { status: 400 });
    }

    console.log('üéØ Smart Transcription:');
    console.log('  - Audio:', audioFile.name, audioFile.size, 'bytes');
    console.log('  - Image:', imageFile ? imageFile.name : 'none');
    console.log('  - Context:', contextText || 'none');
    console.log('  - Mode:', mode);

    // Convert audio to base64
    const audioBytes = await audioFile.arrayBuffer();
    const audioBuffer = Buffer.from(audioBytes);
    const base64Audio = audioBuffer.toString('base64');

    // Prepare multimodal input
    const parts: any[] = [];

    // Build smart prompt based on mode
    let prompt = '';

    if (mode === 'summary') {
      prompt = `ÿßÿ≥ÿ™ŸÖÿπ ÿ•ŸÑŸâ Ÿáÿ∞ÿß ÿßŸÑŸÖŸÑŸÅ ÿßŸÑÿµŸàÿ™Ÿä ŸàÿßŸÉÿ™ÿ® ŸÖŸÑÿÆÿµÿßŸã ÿ¥ÿßŸÖŸÑÿßŸã ŸÑŸÑŸÖÿ≠ÿ™ŸàŸâ:

üìù ÿßŸÑŸÖÿ∑ŸÑŸàÿ®:
- ŸÖŸÑÿÆÿµ ÿ¥ÿßŸÖŸÑ ŸÑŸÑÿ£ŸÅŸÉÿßÿ± ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©
- ÿßŸÑŸÜŸÇÿßÿ∑ ÿßŸÑŸÖŸáŸÖÿ©
- ÿ£Ÿä ÿ™ŸàÿµŸäÿßÿ™ ÿ£Ÿà ÿßÿ≥ÿ™ŸÜÿ™ÿßÿ¨ÿßÿ™

`;
    } else if (mode === 'keywords') {
      prompt = `ÿßÿ≥ÿ™ŸÖÿπ ÿ•ŸÑŸâ Ÿáÿ∞ÿß ÿßŸÑŸÖŸÑŸÅ ÿßŸÑÿµŸàÿ™Ÿä Ÿàÿßÿ≥ÿ™ÿÆÿ±ÿ¨:

üîë ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖŸÅÿ™ÿßÿ≠Ÿäÿ© ŸàÿßŸÑŸÖŸàÿßÿ∂Ÿäÿπ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©
üìä ÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ (ÿ™ÿπŸÑŸäŸÖŸäÿå ÿ™ÿ±ŸÅŸäŸáŸäÿå ÿ•ÿÆÿ®ÿßÿ±Ÿäÿå ÿ•ŸÑÿÆ)
üí° ÿßŸÑÿ£ŸÅŸÉÿßÿ± ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©

`;
    } else {
      // accurate mode
      prompt = `ÿßÿ≥ÿ™ŸÖÿπ ÿ•ŸÑŸâ Ÿáÿ∞ÿß ÿßŸÑŸÖŸÑŸÅ ÿßŸÑÿµŸàÿ™Ÿä ŸàÿßŸÉÿ™ÿ® ÿ™ŸÅÿ±Ÿäÿ∫ÿßŸã ÿØŸÇŸäŸÇÿßŸã ŸÉÿßŸÖŸÑÿßŸã:

üìù ŸÇŸàÿßÿπÿØ ÿßŸÑÿ™ŸÅÿ±Ÿäÿ∫:
- ÿßŸÉÿ™ÿ® ŸÉŸÑ ŸÉŸÑŸÖÿ© ÿ®ÿßŸÑÿ∂ÿ®ÿ∑ ŸÉŸÖÿß ŸÜŸèÿ∑ŸÇÿ™
- ÿ≠ÿßŸÅÿ∏ ÿπŸÑŸâ ÿπŸÑÿßŸÖÿßÿ™ ÿßŸÑÿ™ÿ±ŸÇŸäŸÖ ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ©
- ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑŸÜÿµ ÿ®ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÉÿ™ÿ® ÿ®ÿßŸÑÿπÿ±ÿ®Ÿäÿ©ÿå Ÿàÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿ®ÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ© ÿßŸÉÿ™ÿ® ÿ®ÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ©
- ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸáŸÜÿßŸÉ ŸÖÿµÿ∑ŸÑÿ≠ÿßÿ™ ÿ™ŸÇŸÜŸäÿ©ÿå ÿßŸÉÿ™ÿ®Ÿáÿß ÿ®ÿØŸÇÿ©

`;
    }

    // Add context if provided
    if (contextText) {
      prompt += `\nüìå ÿßŸÑÿ≥ŸäÿßŸÇ: ${contextText}\n\n`;
    }

    // Add image analysis if provided
    if (imageFile) {
      const imageBytes = await imageFile.arrayBuffer();
      const imageBuffer = Buffer.from(imageBytes);
      const base64Image = imageBuffer.toString('base64');

      prompt += `üñºÔ∏è ÿ™ŸÖ ÿ™ÿ≤ŸàŸäÿØŸÉ ÿ®ÿµŸàÿ±ÿ© ŸÖÿ±ŸÅŸÇÿ©. ÿßŸÜÿ∏ÿ± ÿ•ŸÑŸâ ÿßŸÑÿµŸàÿ±ÿ© Ÿàÿßÿ≥ÿ™ÿÆÿØŸÖ ŸÖÿ≠ÿ™ŸàÿßŸáÿß ŸÑŸÅŸáŸÖ ÿ≥ŸäÿßŸÇ ÿßŸÑÿµŸàÿ™ ÿ®ÿ¥ŸÉŸÑ ÿ£ŸÅÿ∂ŸÑ.\n\n`;

      parts.push({
        text: prompt
      });

      parts.push({
        inlineData: {
          data: base64Image,
          mimeType: imageFile.type
        }
      });
    } else {
      parts.push({
        text: prompt
      });
    }

    // Add audio
    parts.push({
      inlineData: {
        data: base64Audio,
        mimeType: audioFile.type || 'audio/webm'
      }
    });

    // Add final instruction
    parts.push({
      text: '\n\n‚úçÔ∏è ÿßŸÑŸÜÿµ:'
    });

    console.log('ü§ñ Processing with Gemini 2.0 Flash (Multimodal)...');

    // Initialize Gemini model
    const model = genAI.getGenerativeModel({
      model: 'gemini-2.0-flash-exp',
      generationConfig: {
        temperature: mode === 'accurate' ? 0.1 : 0.7,
        topK: mode === 'accurate' ? 20 : 40,
        topP: mode === 'accurate' ? 0.8 : 0.95,
      }
    });

    // Generate content
    const result = await model.generateContent(parts);
    const response = await result.response;
    const text = response.text().trim();

    const duration = Date.now() - startTime;

    console.log('‚úÖ Smart transcription successful');
    console.log('üìù Length:', text.length, 'characters');
    console.log('‚è±Ô∏è  Duration:', duration, 'ms');

    // Analyze the response to extract metadata
    const wordCount = text.split(/\s+/).filter(w => w.length > 0).length;
    const hasArabic = /[\u0600-\u06FF]/.test(text);
    const hasEnglish = /[a-zA-Z]/.test(text);
    const language = hasArabic && hasEnglish ? 'mixed' : hasArabic ? 'arabic' : 'english';

    return NextResponse.json({
      success: true,
      text: text,
      metadata: {
        mode: mode,
        language: language,
        wordCount: wordCount,
        hasImage: !!imageFile,
        hasContext: !!contextText,
        duration_ms: duration,
        model: 'gemini-2.0-flash-exp',
        provider: 'Google Gemini Smart'
      }
    });

  } catch (error: any) {
    console.error('‚ùå Smart transcription error:', error);
    console.error('Error details:', error.message);

    return NextResponse.json({
      success: false,
      error: error.message || 'Transcription failed',
      duration_ms: Date.now() - startTime
    }, { status: 500 });
  }
}
